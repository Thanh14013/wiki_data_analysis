# Batch Layer (Historical Analytics) - Technical Documentation

## üìä Overview

Batch Layer x·ª≠ l√Ω d·ªØ li·ªáu historical t·ª´ Data Lake (S3) s·ª≠ d·ª•ng Apache Spark, t·∫°o ra deep analytics views cho long-term insights. Theo Lambda Architecture, layer n√†y t·ªëi ∆∞u cho **high accuracy**, **complex computations**, v√† **comprehensive analysis** h∆°n l√† low latency.

---

## üèóÔ∏è Architecture Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Kafka   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Batch Ingestion ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ S3 Data Lake‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇSpark Processor‚îÇ
‚îÇ(Redpanda)‚îÇ     ‚îÇ  (Archiver)     ‚îÇ     ‚îÇ  (Parquet)  ‚îÇ     ‚îÇ  (PySpark)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ                                            ‚îÇ
                    Buffer 5000                                  Analytics
                    events/60s                                        ‚îÇ
                                                                      ‚ñº
                                                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                             ‚îÇ  PostgreSQL RDS  ‚îÇ
                                                             ‚îÇ Historical Tables‚îÇ
                                                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ Component 1: Batch Ingestion (`ingestion/batch_job.py`)

**Vai tr√≤**: Archive raw events t·ª´ Kafka v√†o S3 Data Lake d∆∞·ªõi d·∫°ng Parquet files, l√†m ngu·ªìn d·ªØ li·ªáu cho Spark batch processing.

### A. Architecture Details

**Consumer Configuration:**
```python
Consumer Group: wiki-batch-archiver-group
Topic: wiki-raw-events
Auto Offset Reset: latest (ch·ªâ l·∫•y new data)
Deserializer: JSON
```

**Why separate consumer group?**
- ƒê·ªôc l·∫≠p v·ªõi Speed Layer (kh√¥ng ·∫£nh h∆∞·ªüng l·∫´n nhau)
- C√≥ th·ªÉ reprocess t·ª´ ƒë·∫ßu m√† kh√¥ng ·∫£nh h∆∞·ªüng real-time
- Kafka tracks offset ri√™ng cho m·ªói group

### B. Buffering Strategy

**Micro-batching approach:**
```python
BATCH_SIZE = 5000 events
TIMEOUT_SEC = 60 seconds

Trigger conditions (OR logic):
1. Buffer reaches 5000 events ‚Üí Flush immediately
2. 60 seconds passed since last flush ‚Üí Flush current buffer

Purpose: Optimize S3 writes (gi·∫£m API calls, gi·∫£m chi ph√≠)
```

**Example Timeline:**
```
T+0s:    Buffer = 0, start collecting
T+30s:   Buffer = 2500 events (ch∆∞a flush)
T+45s:   Buffer = 5000 events ‚Üí FLUSH to S3
T+45.5s: Buffer reset, continue collecting
T+105s:  Buffer = 800 events, 60s timeout ‚Üí FLUSH to S3
```

### C. Data Format & Schema

**Input (from Kafka):**
```json
{
  "timestamp": 1705334400,  // int64 epoch seconds
  "user": "John",
  "title": "Python (programming)",
  "server_name": "en.wikipedia.org",
  "type": "edit",
  "bytes_changed": 500,
  "is_bot": false,
  "is_new_page": false,
  "language": "en",
  "project": "wikipedia",
  "log_params": {"key": "value"},  // Mixed types handled
  ...
}
```

**Critical Type Handling:**
```python
# Problem: timestamp c√≥ th·ªÉ l√† string ho·∫∑c int t·ª´ producer
# Solution: Force convert to int64 before buffering
if not isinstance(event['timestamp'], int):
    event['timestamp'] = int(event['timestamp'])

# Problem: log_params c√≥ th·ªÉ l√† dict, list, ho·∫∑c string
# Solution: Normalize to JSON string
if isinstance(x, (dict, list)):
    return json.dumps(x, ensure_ascii=False)
```

**Output Format (Parquet):**
```
Compression: Snappy (fast compression/decompression)
Version: 2.6 (compatible v·ªõi Spark 3.x)
Schema: Inferred from DataFrame
Encoding: Dictionary encoding for strings (automatic)
```

### D. S3 Storage Structure

**Partitioning Strategy:**
```
s3://wiki-data-lake-prod-1407/raw_data/raw_events/
‚îú‚îÄ‚îÄ year=2026/
‚îÇ   ‚îú‚îÄ‚îÄ month=01/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wiki_events_20260115_070916.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wiki_events_20260115_071420.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wiki_events_20260115_071520.parquet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ month=02/
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ year=2027/
    ‚îî‚îÄ‚îÄ ...
```

**Benefits c·ªßa partitioning:**
- Spark c√≥ th·ªÉ **partition pruning** (ch·ªâ ƒë·ªçc data c·∫ßn thi·∫øt)
- Query theo ng√†y/th√°ng r·∫•t nhanh
- Cleanup d·ªÖ d√†ng (x√≥a theo month)
- Tu√¢n th·ªß Hive partitioning convention

**File naming convention:**
```
Format: wiki_events_{YYYYMMDD}_{HHMMSS}.parquet
Example: wiki_events_20260115_071420.parquet

Timestamp: UTC time khi file ƒë∆∞·ª£c t·∫°o
Unique: ƒê·∫£m b·∫£o kh√¥ng overwrite
```

### E. Write Process

**Local vs S3:**

```python
if data_lake_path.startswith("s3://") or data_lake_path.startswith("s3a://"):
    # Production: S3 upload
    1. Convert DataFrame to PyArrow Table
    2. Write to in-memory buffer (BytesIO)
    3. Upload buffer to S3 via boto3
    4. Log success with full S3 path
else:
    # Development: Local filesystem
    1. Create directory structure
    2. Write Parquet directly to disk
    3. Log local path
```

**S3 Client Configuration:**
```python
# IAM Role on EC2 (Production - Recommended)
- Kh√¥ng c·∫ßn access keys
- Automatic credential rotation
- Permissions managed centrally

# Explicit credentials (Development/Testing)
- AWS_ACCESS_KEY_ID
- AWS_SECRET_ACCESS_KEY
- Manual key rotation required
```

**Write Performance:**
```
File size: ~500KB - 5MB per file (5000 events)
Compression ratio: 5-10x (JSON ‚Üí Parquet)
Write latency: 
  - Local: 50-200ms
  - S3: 500ms - 2s (network dependent)
Throughput: 2000-10000 events/second
```

### F. Error Handling & Reliability

**Retry Strategy:**
```python
# Kafka consumer auto-commit AFTER successful write
# If S3 write fails:
1. Log error with full traceback
2. Buffer is NOT cleared
3. Kafka offset NOT committed
4. Next poll will retry same events

Risk: Memory overflow n·∫øu S3 down l√¢u
Mitigation: Container restart, offset manual reset
```

**Data Quality Checks:**
```python
# Before write:
1. DataFrame not empty
2. Schema validation (implicit in PyArrow)
3. Type consistency (timestamp as int64)

# After write:
1. Log record count
2. Verify S3 object exists (could be added)
```

**Monitoring Metrics:**
```
Success metrics:
- ‚úÖ Uploaded N records to s3://bucket/path
- File count per hour
- Data volume written (MB/hour)

Failure metrics:
- ‚ùå S3 Upload Failed: {error}
- Failed write attempts
- Time since last successful write
```

---

## üéØ Component 2: Batch Processor (`processing/batch_job.py`)

**Vai tr√≤**: ƒê·ªçc data t·ª´ S3 Data Lake, ch·∫°y complex analytics v·ªõi Spark, ghi k·∫øt qu·∫£ v√†o PostgreSQL historical tables.

### A. Spark Session Configuration

**Dependencies Management:**
```python
Spark Version: 3.5.1
Scala Version: 2.12
Hadoop Version: 3.3.4

Required JARs:
1. spark-sql-kafka-0-10_2.12:3.5.1  # Kafka integration
2. postgresql:42.6.0                  # JDBC driver
3. commons-pool2:2.11.1               # Connection pooling
4. hadoop-aws:3.3.4                   # S3 support
5. aws-java-sdk-bundle:1.12.262       # AWS SDK

Auto-download: Maven Central (first run takes 3-5 mins)
```

**Spark Configuration:**
```python
Master: local[*]  # Use all available cores
App Name: WikiDataPipeline-Batch

Optimizations:
- spark.sql.adaptive.enabled = true
  ‚Üí Dynamic partition coalescing
  ‚Üí Runtime query optimization
  
- spark.sql.adaptive.coalescePartitions.enabled = true
  ‚Üí Reduce partitions after shuffle
  
- spark.sql.shuffle.partitions = 200
  ‚Üí Default parallelism for joins/aggregations

Log Level: WARN (reduce noise)
```

**S3 Access Configuration:**
```python
# S3A FileSystem (Hadoop 3.x)
fs.s3a.impl = org.apache.hadoop.fs.s3a.S3AFileSystem

# Credentials Provider Chain
Priority:
1. Instance Profile (EC2 IAM Role) ‚Üê Recommended
2. Environment variables
3. Explicit config in code

# Performance tuning (could be added):
fs.s3a.connection.maximum = 100
fs.s3a.threads.max = 64
fs.s3a.fast.upload = true
```

### B. Data Reading Strategy

**Path Pattern:**
```python
# Before fix: s3a://.../raw_events (kh√¥ng t√¨m th·∫•y data)
# After fix:  s3a://.../raw_events/*/* (wildcard partitions)

Pattern: year=*/month=*
Example matches:
- year=2026/month=01/*.parquet
- year=2026/month=02/*.parquet
- year=2027/month=01/*.parquet

basePath: s3a://.../raw_events
‚Üí Spark t·ª± ƒë·ªông detect partition columns (year, month)
```

**Schema Handling:**
```python
.option("mergeSchema", "true")
‚Üí Handle schema evolution
‚Üí Merge schemas from multiple files
‚Üí Add new columns as nullable

Example:
File 1: (timestamp, user, title)
File 2: (timestamp, user, title, new_column)
Result: All columns merged, new_column nullable
```

**Timestamp Conversion:**
```python
# Parquet has: timestamp as int64 (epoch seconds)
# Spark needs: timestamp as TimestampType

Convert:
df = df.withColumn("timestamp", 
                   col("timestamp").cast("timestamp"))

Purpose: Enable time-based operations:
- date_trunc("hour", timestamp)
- Filter by date ranges
- Window functions with time ordering
```

**Date Filtering:**
```python
days_back = 1  # Configurable parameter

Filter logic:
cutoff_date = datetime.now() - timedelta(days=days_back)
df = df.filter(col("timestamp") >= cutoff_date)

Purpose:
- Process only recent data
- Incremental processing (daily job)
- Reduce compute cost
```

### C. Analytics Computations

#### 1. **Hourly Patterns** (`historical_hourly_patterns`)

**Purpose**: Time-series c·ªßa edit activity m·ªói gi·ªù

```python
Aggregation:
df.withColumn("hour_bucket", date_trunc("hour", col("timestamp")))
  .groupBy("hour_bucket")
  .agg(
      count("*").alias("total_events"),
      sum("bytes_changed").alias("total_bytes"),
      avg("bytes_changed").alias("avg_bytes"),
      count(col("is_bot") == True).alias("bot_events"),
      count(col("is_bot") == False).alias("human_events")
  )
  .orderBy("hour_bucket")

Output: 24-168 rows (1-7 days of hourly data)
```

**Use case**: Dashboard time-series chart

#### 2. **Hourly Trends** (`historical_hourly_trends`)

**Purpose**: Hourly metrics v·ªõi 24-hour moving average

```python
Step 1: Calculate hourly aggregates
hourly_trends = df.groupBy(date_trunc("hour", "timestamp"))
                  .agg(total_events, total_bytes, bot_count, ...)

Step 2: Add moving averages
window_spec = Window.orderBy("hour_bucket").rowsBetween(-23, 0)

result = hourly_trends.withColumn(
    "events_24h_avg", avg("total_events").over(window_spec)
).withColumn(
    "bytes_24h_avg", avg("total_bytes").over(window_spec)
)

Window explanation:
- rowsBetween(-23, 0): Current row + previous 23 rows = 24 hours
- Sliding window for smoothing trends
```

**Use case**: Trend analysis v·ªõi noise reduction

#### 3. **Top Contributors** (`historical_top_contributors`)

**Purpose**: Identify most active users/bots

```python
df.groupBy("user", "is_bot")
  .agg(
      count("*").alias("edit_count"),
      sum("bytes_changed").alias("total_bytes"),
      count(col("is_new_page") == True).alias("pages_created")
  )
  .orderBy(desc("edit_count"))
  .limit(100)

Output: Top 100 users ranked by edit count
```

**Use case**: Leaderboard, power user identification

#### 4. **Language Distribution** (`historical_language_distribution`)

**Purpose**: Edit activity across languages/projects

```python
df.groupBy("language", "project")
  .agg(
      count("*").alias("edit_count"),
      sum("bytes_changed").alias("total_bytes"),
      count(col("is_bot") == True).alias("bot_edits"),
      count(col("is_new_page") == True).alias("new_pages")
  )
  .orderBy(desc("edit_count"))

Output: ~50-200 rows (active language-project combinations)
```

**Use case**: Global activity heatmap, language comparison

#### 5. **Server Rankings** (`historical_server_rankings`)

**Purpose**: Rank wikis by activity v·ªõi percentile scores

```python
Step 1: Aggregate by server
server_stats = df.groupBy("server_name", "language", "project")
                 .agg(
                     count("*").alias("edit_count"),
                     sum("bytes_changed").alias("total_bytes"),
                     countDistinct("user").alias("unique_users")
                 )

Step 2: Add rankings
window_spec = Window.orderBy(desc("edit_count"))

ranked = server_stats.withColumn("rank", row_number().over(window_spec))
                     .withColumn("percentile", percent_rank().over(window_spec))
                     .filter(col("rank") <= 50)

Output: Top 50 servers v·ªõi rank v√† percentile
```

**Use case**: Server comparison, identify most active wikis

### D. Write to PostgreSQL

**JDBC Write Configuration:**
```python
df.write \
  .format("jdbc") \
  .option("url", "jdbc:postgresql://host:5432/db") \
  .option("dbtable", table_name) \
  .option("user", user) \
  .option("password", password) \
  .option("driver", "org.postgresql.Driver") \
  .mode("overwrite") \  # Replace existing data
  .save()
```

**Write Strategy:**
```
Mode: overwrite
‚Üí Truncate table before insert
‚Üí Historical tables are "views" not append logs
‚Üí Always contain latest batch result

Alternative modes:
- append: Add to existing data (for incremental)
- ignore: Skip if table exists
- error: Fail if table exists (default)
```

**Performance:**
```
Parallelism: Spark partitions ‚Üí JDBC connections
Default: 200 partitions (may be overkill)
Optimization: Coalesce to 1-10 partitions before write

Write speed: ~1000-10000 rows/second
Depends on:
- Network latency
- Database CPU
- Row size
- Index count
```

### E. Execution Schedule

#### Current Implementation: Docker Compose Loop

**Default Configuration (docker-compose.yml):**
```yaml
batch-processor:
  command: >
    bash -c "while true; do 
      python processing/batch_job.py --days ${BATCH_DAYS:-2}; 
      sleep ${BATCH_INTERVAL_SECONDS:-3600}; 
    done"
  environment:
    - BATCH_DAYS=2              # Process last 2 days
    - BATCH_INTERVAL_SECONDS=3600  # Run every 1 hour
```

**Scheduling Behavior:**

```
Timeline:
T+0s:        Container starts, ch·∫°y l·∫ßn ƒë·∫ßu ngay l·∫≠p t·ª©c
T+180s:      L·∫ßn ƒë·∫ßu ho√†n th√†nh (~3 ph√∫t cho 10k events)
T+180s:      Sleep 3600s (1 gi·ªù)
T+3780s:     Wake up, ch·∫°y l·∫ßn 2
T+3960s:     L·∫ßn 2 ho√†n th√†nh
T+3960s:     Sleep 3600s
T+7560s:     Wake up, ch·∫°y l·∫ßn 3
...
```

**‚ö†Ô∏è Important Notes:**
- L·∫ßn ƒë·∫ßu run NGAY khi container start (kh√¥ng ch·ªù 1 gi·ªù)
- Interval ƒë∆∞·ª£c t√≠nh T·ª™ KHI k·∫øt th√∫c job tr∆∞·ªõc, KH√îNG ph·∫£i t·ª´ l√∫c b·∫Øt ƒë·∫ßu
- N·∫øu job ch·∫°y 5 ph√∫t ‚Üí actual interval = 3600s + 300s = ~1h5m gi·ªØa 2 l·∫ßn start

---

#### Tuning Scheduling Frequency

**Option 1: Environment Variables (.env file)**

```bash
# Ch·∫°y m·ªói 15 ph√∫t (cho development/testing)
BATCH_INTERVAL_SECONDS=900
BATCH_DAYS=1  # Reduce lookback window

# Ch·∫°y m·ªói 6 gi·ªù (cho production v·ªõi √≠t data)
BATCH_INTERVAL_SECONDS=21600
BATCH_DAYS=7  # Process longer history

# Ch·∫°y m·ªói ng√†y l√∫c 2 AM (use cron instead)
BATCH_INTERVAL_SECONDS=86400
BATCH_DAYS=7
```

**Restart ƒë·ªÉ apply changes:**
```bash
docker compose restart batch-processor
docker compose logs -f batch-processor  # Verify new interval
```

**Option 2: Manual Trigger (Kh√¥ng ƒë·ª£i interval)**

```bash
# Trigger ngay l·∫≠p t·ª©c, kh√¥ng ·∫£nh h∆∞·ªüng schedule
docker exec wiki-batch-processor python processing/batch_job.py --days 2

# Check k·∫øt qu·∫£
psql -c "SELECT MAX(hour_bucket) FROM historical_hourly_patterns;"
```

**Option 3: Cron-based Scheduling (Alternative)**

```bash
# Host cron (kh√¥ng d√πng docker loop)
# /etc/crontab
0 * * * * docker exec wiki-batch-processor python processing/batch_job.py --days=1

# Ho·∫∑c inside container cron
0 */6 * * * cd /app && python processing/batch_job.py --days=3
```

---

#### Frequency Trade-offs

| Interval | Pros | Cons | Use Case |
|---------|------|------|----------|
| **15 min** | ‚Ä¢ Fresh data<br>‚Ä¢ Quick iterations | ‚Ä¢ High S3 API costs<br>‚Ä¢ Wasted compute if no new data | Development, testing |
| **1 hour** ‚≠ê | ‚Ä¢ Balanced freshness<br>‚Ä¢ Reasonable costs<br>‚Ä¢ Smooth dashboard updates | ‚Ä¢ 1 hour delay for insights | **Production (recommended)** |
| **6 hours** | ‚Ä¢ Very low costs<br>‚Ä¢ Efficient batching | ‚Ä¢ Stale historical views | Low-traffic sites |
| **Daily** | ‚Ä¢ Cheapest<br>‚Ä¢ Large batch efficiency | ‚Ä¢ Historical lags 24h | Archive-only mode |

**Cost Example (S3 + Spark):**
```
15-min interval: 96 runs/day √ó $0.02 = $1.92/day = $58/month
1-hour interval: 24 runs/day √ó $0.02 = $0.48/day = $14/month ‚úÖ
6-hour interval: 4 runs/day √ó $0.02 = $0.08/day = $2.4/month
```

---

#### Best Practice: Apache Airflow (Future)

**For production-grade orchestration:**

```python
from airflow import DAG
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.sensors.external_task import ExternalTaskSensor
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'wiki_batch_processing',
    default_args=default_args,
    description='Batch process Wikipedia historical data',
    schedule_interval='@hourly',  # Cron: 0 * * * *
    start_date=datetime(2026, 1, 1),
    catchup=False,  # Don't backfill
    max_active_runs=1,  # No concurrent runs
)

# Task 1: Check if S3 has new data
check_s3 = DockerOperator(
    task_id='check_new_s3_data',
    image='wiki-processing:latest',
    command='python scripts/check_s3_freshness.py',
    dag=dag,
)

# Task 2: Run Spark job
process_batch = DockerOperator(
    task_id='run_batch_processor',
    image='wiki-processing:latest',
    command='python processing/batch_job.py --days=1',
    environment={
        'POSTGRES_HOST': '{{ var.value.db_host }}',
        'AWS_REGION': 'us-east-1',
    },
    dag=dag,
)

# Task 3: Data quality checks
validate_output = DockerOperator(
    task_id='validate_historical_tables',
    image='wiki-processing:latest',
    command='python scripts/validate_batch_output.py',
    dag=dag,
)

check_s3 >> process_batch >> validate_output
```

**Benefits:**
- ‚úÖ Dependency management (check S3 before processing)
- ‚úÖ Retry logic v·ªõi exponential backoff
- ‚úÖ Monitoring dashboard
- ‚úÖ Alerting on failures
- ‚úÖ Backfill support
- ‚úÖ Skip runs if no new data

### F. Performance Characteristics

**Resource Usage:**
```
Spark Driver:
- Memory: 1-2GB
- CPU: 2-4 cores
- Disk: Minimal (streaming read from S3)

Processing Time:
- 10K events: ~30 seconds
- 100K events: ~2-3 minutes
- 1M events: ~15-20 minutes

Bottlenecks:
- S3 read: Network bandwidth
- Aggregations: CPU (DataFrame operations)
- JDBC write: Database connections
```

**Cost Optimization:**
```
EC2 instance sizing:
- Development: t3.medium (2 vCPU, 4GB)
- Production: m5.xlarge (4 vCPU, 16GB)
- High volume: m5.2xlarge (8 vCPU, 32GB)

S3 costs:
- Storage: $0.023/GB/month
- GET requests: $0.0004/1000
- Data transfer: Free to same region EC2

Tip: Use S3 Intelligent-Tiering for auto archival
```

---

## üîÑ End-to-End Batch Flow

### Timeline: Kafka ‚Üí Dashboard

```
T+0s:        Events arriving in Kafka
T+30s:       Batch ingestion buffering (2500 events)
T+60s:       Buffer timeout ‚Üí Flush 2500 events to S3
T+61s:       Parquet file written: year=2026/month=01/wiki_events_xxx.parquet
T+3600s:     Batch processor cron triggers (hourly)
T+3610s:     Spark reads S3, processes 50K events
T+3700s:     Aggregations complete (5 tables)
T+3750s:     JDBC writes to PostgreSQL complete
T+3751s:     Dashboard Historical Analytics page ready
T+3760s:     User clicks "Historical Analytics" ‚Üí sees new data
```

**Total latency: ~1 hour** (configurable: c√≥ th·ªÉ ch·∫°y m·ªói 15 ph√∫t)

---

## üìà Scalability & Performance

### Current Capacity:

```
Batch Ingestion:
- Throughput: 5000 events/60s = ~83 events/s write to S3
- Kafka lag: Minimal (ingestion faster than stream rate)
- S3 write frequency: 1 file/minute peak, 1 file/5min average

Batch Processor:
- Data volume: 1-7 days of history
- Processing: 100K events in 2-3 minutes
- Tables updated: 5 tables, 300-500 rows total
```

### Scaling Strategies:

**1. Increase Ingestion Frequency:**
```python
Current: 5000 events OR 60s
Optimized: 10000 events OR 120s
‚Üí Fewer S3 files, lower API costs
‚Üí Larger files, better compression
```

**2. Partition Parallelism:**
```python
# Add to Spark config
.config("spark.sql.shuffle.partitions", "50")
.config("spark.default.parallelism", "20")

# Repartition before heavy operations
df = df.repartition(10, "server_name")
```

**3. Incremental Processing:**
```python
# Instead of --days=7 full reprocess
# Track last processed timestamp
last_run = get_last_run_timestamp()
df = df.filter(col("timestamp") > last_run)

# Write in append mode
.mode("append")
```

**4. Separate Compute & Storage:**
```
Current: Spark on same EC2 as other services
Future: 
- Dedicated EMR cluster for Spark jobs
- Spot instances for cost savings
- Auto-scaling based on queue depth
```

---

## üõ°Ô∏è Fault Tolerance & Reliability

### Data Durability:

**S3 Storage:**
```
Durability: 99.999999999% (11 nines)
Replication: Cross-AZ automatic
Versioning: Can be enabled (not currently)
Lifecycle: Can auto-archive to Glacier
```

**Recovery Scenarios:**

**1. Batch Ingestion Crash:**
```
Impact: Events kh√¥ng ƒë∆∞·ª£c archived
Recovery: Kafka offset ch∆∞a commit ‚Üí restart s·∫Ω replay
Data Loss: None (events v·∫´n trong Kafka 24h retention)
```

**2. S3 Write Failure:**
```
Impact: Buffer m·∫•t n·∫øu container crash
Mitigation: 
- Kafka offset auto-commit sau write success
- Failed events s·∫Ω ƒë∆∞·ª£c retry t·ª´ Kafka
- Max data loss: 1 buffer (5000 events ~2-3 ph√∫t)
```

**3. Batch Processor Failure:**
```
Impact: Historical tables kh√¥ng ƒë∆∞·ª£c update
Recovery: Re-run job manually
Data Loss: None (source data v·∫´n trong S3)
Mitigation: Airflow retry policy
```

**4. S3 Partition Corruption:**
```
Impact: M·ªôt partition kh√¥ng ƒë·ªçc ƒë∆∞·ª£c
Recovery: Spark skip bad files v·ªõi option
Data Loss: Limited to corrupt partition
Prevention: Checksum verification, S3 versioning
```

### Monitoring & Alerts:

**Ingestion Metrics:**
```python
‚úÖ Success:
- "Uploaded N records to s3://..."
- File count per hour (expected: 60-120)
- Kafka consumer lag (<1000)

‚ùå Failures:
- "S3 Upload Failed: {error}"
- No files written for >5 minutes
- Kafka lag > 10000
```

**Processor Metrics:**
```python
‚úÖ Success:
- "‚úÖ Batch processing completed successfully!"
- "‚úÖ Loaded N records from Data Lake"
- "‚úÖ Wrote N records to table_name"

‚ùå Failures:
- "‚ö†Ô∏è Data lake path not found"
- "‚ùå Batch processing failed: {error}"
- Job duration > 30 minutes
```

**Database Health:**
```sql
-- Check last update time
SELECT MAX(hour_bucket) FROM historical_hourly_patterns;

-- Should be within last 2 hours
-- If older ‚Üí batch processor not running
```

---

## üîß Configuration & Tuning

### Environment Variables:

```bash
# S3 Data Lake
DATA_LAKE_PATH=s3a://wiki-data-lake-prod-1407/raw_data
AWS_REGION=us-east-1
S3_BUCKET_NAME=wiki-data-lake-prod-1407
AWS_ACCESS_KEY_ID=  # Empty = use IAM role
AWS_SECRET_ACCESS_KEY=

# Database
POSTGRES_HOST=wiki-pipeline-db.xxx.rds.amazonaws.com
POSTGRES_PORT=5432
POSTGRES_DB=wikidb
POSTGRES_USER=thanh123
POSTGRES_PASSWORD=***

# Spark
SPARK_MASTER=local[*]  # Use all cores
CHECKPOINT_PATH=/tmp/checkpoints
```

### Performance Tuning Parameters:

**Batch Ingestion:**
```python
# Memory vs Latency tradeoff
BATCH_SIZE = 5000     # Standard
BATCH_SIZE = 10000    # Better compression, lower API costs
BATCH_SIZE = 2000     # Lower latency, more files

TIMEOUT_SEC = 60      # Standard
TIMEOUT_SEC = 300     # For low-traffic periods
TIMEOUT_SEC = 30      # For high-frequency updates
```

**Spark Configuration:**
```python
# For large datasets (1M+ events)
.config("spark.executor.memory", "4g")
.config("spark.driver.memory", "2g")
.config("spark.sql.shuffle.partitions", "100")

# For memory-constrained environments
.config("spark.executor.memory", "1g")
.config("spark.driver.memory", "1g")
.config("spark.sql.shuffle.partitions", "20")
```

**JDBC Write Tuning:**
```python
# Reduce partitions before write (avoid too many connections)
df.coalesce(5).write.jdbc(...)

# Batch insert size
.option("batchsize", 10000)

# Isolation level
.option("isolationLevel", "READ_UNCOMMITTED")
```

---

## üìä Cost Analysis

### Monthly Costs (Assuming 100M events/month):

**S3 Storage:**
```
Data volume: 100M events √ó 500 bytes/event = 50GB uncompressed
After compression (5x): 10GB
Cost: 10GB √ó $0.023/GB = $0.23/month

API calls: 
- PUT: 10,000 files √ó $0.005/1000 = $0.05/month
- GET: 100 Spark reads √ó $0.0004/1000 = negligible

Total S3: ~$0.30/month
```

**EC2 Compute (t3.medium, on-demand):**
```
Price: $0.0416/hour
Monthly: $0.0416 √ó 24 √ó 30 = $29.95/month

Alternative (Spot):
Price: ~$0.012/hour (70% savings)
Monthly: ~$9/month
```

**RDS PostgreSQL (db.t3.micro):**
```
Price: $0.017/hour
Monthly: $0.017 √ó 24 √ó 30 = $12.24/month

Storage: 20GB √ó $0.115/GB = $2.30/month
Total RDS: ~$14.54/month
```

**Total Monthly Cost: ~$44/month** (with on-demand EC2)  
**Optimized: ~$24/month** (with Spot instances)

---

## üöÄ Operational Procedures

### Daily Operations:

**Morning Check:**
```bash
# Check last batch run
docker compose logs batch-processor --tail=50

# Verify data freshness
psql -c "SELECT MAX(hour_bucket) FROM historical_hourly_patterns;"

# Check S3 file count (should increase daily)
aws s3 ls s3://bucket/raw_data/raw_events/year=2026/month=01/ --recursive | wc -l
```

**Weekly Maintenance:**
```bash
# Check S3 storage size
aws s3 ls s3://bucket/raw_data/ --recursive --summarize | grep "Total Size"

# Vacuum historical tables
psql -c "VACUUM ANALYZE historical_hourly_patterns;"
```

**Monthly Tasks:**
```sql
-- Archive old data (optional)
-- Copy old partitions to Glacier
aws s3 sync s3://bucket/raw_data/raw_events/year=2025/ \
            s3://bucket-archive/year=2025/ \
            --storage-class GLACIER

-- Delete after verification
aws s3 rm s3://bucket/raw_data/raw_events/year=2025/ --recursive
```

### Troubleshooting:

**1. No new historical data:**
```bash
# Check batch processor container
docker compose ps batch-processor  # Should be Up

# Check logs
docker compose logs batch-processor --tail=100

# Manual run
docker compose exec batch-processor python processing/batch_job.py --days=1
```

**2. S3 permission errors:**
```bash
# Test S3 access
aws s3 ls s3://bucket/raw_data/

# Check IAM role (on EC2)
aws sts get-caller-identity

# Verify bucket policy
aws s3api get-bucket-policy --bucket wiki-data-lake-prod-1407
```

**3. Spark out of memory:**
```python
# Symptom: "java.lang.OutOfMemoryError: GC overhead limit exceeded"

# Solution 1: Increase memory
.config("spark.driver.memory", "4g")

# Solution 2: Reduce data range
python processing/batch_job.py --days=1  # Instead of --days=7

# Solution 3: Increase partitions
.config("spark.sql.shuffle.partitions", "50")
```

**4. Slow Spark jobs:**
```bash
# Check Spark UI (if enabled)
http://localhost:4040

# Look for:
- Skewed partitions (one partition much larger)
- High GC time (memory pressure)
- Shuffle read/write size (optimize joins)

# Solutions:
- Repartition data: df.repartition(20)
- Broadcast small tables: broadcast(df_small)
- Cache intermediate results: df.cache()
```

---

## üéØ Future Enhancements

### Short-term (1-3 months):

1. **Schema Evolution**: Properly handle new fields without reprocessing
2. **Data Quality**: Great Expectations integration for validation
3. **Incremental Processing**: Only process new data since last run
4. **Alerting**: Email/Slack notifications on failures

### Medium-term (3-6 months):

1. **Delta Lake**: Replace Parquet v·ªõi Delta format
   - ACID transactions
   - Time travel queries
   - Schema enforcement
   
2. **Apache Airflow**: Proper orchestration
   - Dependency management
   - Retry logic
   - SLA monitoring
   
3. **AWS EMR**: Dedicated Spark cluster
   - Auto-scaling
   - Spot instances
   - Better performance

### Long-term (6-12 months):

1. **Real-time Sync**: Lambda Layer serving merge
2. **ML Pipeline**: Feature engineering on Spark
3. **Data Catalog**: AWS Glue for metadata management
4. **Query Engine**: Athena/Presto for ad-hoc analysis

---

## üìö References

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Parquet Format Specification](https://parquet.apache.org/docs/)
- [AWS S3 Best Practices](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html)
- [Spark on S3 Performance](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-s3-optimized-committer.html)

---

**Document Version**: 1.0  
**Last Updated**: January 15, 2026  
**Author**: thanh123  
**Status**: Production Ready ‚úÖ
