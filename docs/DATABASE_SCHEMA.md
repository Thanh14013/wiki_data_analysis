# Database Schema Documentation - Wiki Data Analysis System

## üìö Overview

T√†i li·ªáu n√†y li·ªát k√™ **chi ti·∫øt t·∫•t c·∫£ c√°c b·∫£ng** trong h·ªá th·ªëng Wiki Data Analysis, bao g·ªìm:
- C√°c database ƒë∆∞·ª£c s·ª≠ d·ª•ng
- Schema c·ªßa t·ª´ng b·∫£ng
- Ngu·ªìn d·ªØ li·ªáu v√† c∆° ch·∫ø ghi
- Chu k·ª≥ c·∫≠p nh·∫≠t
- M·ª•c ƒë√≠ch s·ª≠ d·ª•ng

---

## üóÑÔ∏è Databases Overview

H·ªá th·ªëng s·ª≠ d·ª•ng **2 lo·∫°i database/storage**:

### 1. **PostgreSQL (RDS) - Analytical Database**
- **Endpoint**: `wiki-pipeline-db.cen6acgi49ye.us-east-1.rds.amazonaws.com:5432`
- **Database name**: `wikidb`
- **Region**: `us-east-1`
- **Purpose**: L∆∞u tr·ªØ d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω cho Dashboard v√† Analytics
- **Tables**: 7 tables (2 realtime + 5 historical)

### 2. **Amazon S3 - Data Lake**
- **Bucket**: `wiki-data-lake-prod-1407`
- **Format**: Parquet files (columnar format)
- **Partitioning**: `raw_events/year=YYYY/month=MM/`
- **Purpose**: Archive raw events cho batch processing
- **Not a traditional database**: D·ªØ li·ªáu ƒë∆∞·ª£c t·ªï ch·ª©c d·∫°ng files, kh√¥ng c√≥ tables

---

## üìä PostgreSQL Tables - Complete List

### **A. Real-time Tables (Speed Layer)** 

C√°c b·∫£ng n√†y ƒë∆∞·ª£c c·∫≠p nh·∫≠t **real-time** b·ªüi Quix Stream Processor t·ª´ Kafka stream.

---

#### 1. `realtime_traffic_volume`

**üìù M√¥ t·∫£**: Th·ªëng k√™ traffic volume theo th·ªùi gian (time-series), hi·ªÉn th·ªã m·ª©c ƒë·ªô ho·∫°t ƒë·ªông tr√™n Wikipedia theo t·ª´ng c·ª≠a s·ªï th·ªùi gian.

**üèóÔ∏è Schema**:
```sql
CREATE TABLE realtime_traffic_volume (
    window_start TIMESTAMP,           -- Th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu c·ª≠a s·ªï
    total_bytes BIGINT,                -- T·ªïng bytes thay ƒë·ªïi trong window
    event_count INT,                   -- S·ªë l∆∞·ª£ng events trong window
    avg_bytes_per_event FLOAT          -- Trung b√¨nh bytes/event (GENERATED)
        GENERATED ALWAYS AS (
            CASE WHEN event_count > 0 
            THEN total_bytes::FLOAT / event_count 
            ELSE 0 END
        ) STORED
);

-- Index for performance
CREATE INDEX idx_traffic_window ON realtime_traffic_volume(window_start);
```

**üìä C·∫•u tr√∫c d·ªØ li·ªáu**:
| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `window_start` | TIMESTAMP | Th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu window | `2026-01-19 14:30:00` |
| `total_bytes` | BIGINT | T·ªïng bytes changed | `125000` |
| `event_count` | INT | S·ªë events | `100` |
| `avg_bytes_per_event` | FLOAT | TB bytes/event | `1250.0` |

**‚úçÔ∏è C∆° ch·∫ø ghi d·ªØ li·ªáu**:

| Thu·ªôc t√≠nh | Chi ti·∫øt |
|------------|----------|
| **Component ghi** | `processing/quix_job.py` - Quix Stream Processor |
| **V·ªã tr√≠ trong code** | [processing/quix_job.py](../processing/quix_job.py#L158) - H√†m `main()` |
| **Ngu·ªìn d·ªØ li·ªáu** | Kafka topic `wiki-raw-events` |
| **Chu k·ª≥ ghi** | **Micro-batch**: Ghi m·ªói khi ƒë·ªß 100 events t·ª´ Kafka (~10-30 gi√¢y t√πy traffic) |
| **Ph∆∞∆°ng th·ª©c** | `INSERT` batch s·ª≠ d·ª•ng `psycopg2.extras.execute_values()` |
| **Logic t·ªïng h·ª£p** | T√≠nh t·ªïng `total_bytes` v√† ƒë·∫øm `event_count` t·ª´ buffer 100 events |
| **Retention** | **24 gi·ªù** - Auto cleanup m·ªói 100 polls (~2 ph√∫t) x√≥a data c≈© h∆°n 24h |

**üîÑ Flow ghi d·ªØ li·ªáu**:
```
Kafka (wiki-raw-events)
    ‚Üì [consume messages]
Quix Job Buffer (in-memory)
    ‚Üì [buffer 100 events]
Calculate Aggregations:
    - total_bytes = sum(bytes_changed)
    - event_count = 100
    - window_start = NOW()
    ‚Üì [execute_values()]
PostgreSQL: realtime_traffic_volume
```

**üéØ M·ª•c ƒë√≠ch s·ª≠ d·ª•ng**:
- Dashboard: Real-time traffic chart (line graph theo th·ªùi gian)
- Alert system: Ph√°t hi·ªán traffic spikes b·∫•t th∆∞·ªùng
- Capacity planning: Monitor load patterns

**üßπ Cleanup Policy**:
```sql
-- Auto-cleanup trong quix_job.py, ch·∫°y m·ªói ~2 ph√∫t
DELETE FROM realtime_traffic_volume 
WHERE window_start < NOW() - INTERVAL '24 HOURS';
```

---

#### 2. `realtime_recent_changes`

**üìù M√¥ t·∫£**: Feed raw c·ªßa c√°c thay ƒë·ªïi g·∫ßn ƒë√¢y tr√™n Wikipedia, hi·ªÉn th·ªã chi ti·∫øt t·ª´ng edit/change event.

**üèóÔ∏è Schema**:
```sql
CREATE TABLE realtime_recent_changes (
    event_time TIMESTAMP,              -- Th·ªùi ƒëi·ªÉm event x·∫£y ra
    "user" TEXT,                       -- Username (quoted v√¨ "user" l√† reserved)
    title TEXT,                        -- Ti√™u ƒë·ªÅ b√†i vi·∫øt
    server_name TEXT,                  -- Wiki server (vd: en.wikipedia.org)
    is_bot BOOLEAN,                    -- User l√† bot hay kh√¥ng
    type TEXT,                         -- Lo·∫°i event: edit/new/log
    bytes_changed BIGINT,              -- S·ªë bytes thay ƒë·ªïi
    length_diff INT                    -- Ch√™nh l·ªách ƒë·ªô d√†i (new - old)
);

-- Index for performance
CREATE INDEX idx_changes_time ON realtime_recent_changes(event_time);
```

**üìä C·∫•u tr√∫c d·ªØ li·ªáu**:
| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `event_time` | TIMESTAMP | Th·ªùi ƒëi·ªÉm event | `2026-01-19 14:30:15` |
| `"user"` | TEXT | Username | `JohnEditor123` |
| `title` | TEXT | T√™n b√†i vi·∫øt | `Python (programming)` |
| `server_name` | TEXT | Wiki server | `en.wikipedia.org` |
| `is_bot` | BOOLEAN | Bot flag | `false` |
| `type` | TEXT | Event type | `edit` / `new` / `log` |
| `bytes_changed` | BIGINT | Bytes changed | `500` |
| `length_diff` | INT | Length diff | `+250` (c√≥ th·ªÉ √¢m) |

**‚úçÔ∏è C∆° ch·∫ø ghi d·ªØ li·ªáu**:

| Thu·ªôc t√≠nh | Chi ti·∫øt |
|------------|----------|
| **Component ghi** | `processing/quix_job.py` - Quix Stream Processor |
| **V·ªã tr√≠ trong code** | [processing/quix_job.py](../processing/quix_job.py#L162-L166) - H√†m `main()` |
| **Ngu·ªìn d·ªØ li·ªáu** | Kafka topic `wiki-raw-events` |
| **Chu k·ª≥ ghi** | **Micro-batch**: Ghi m·ªói khi ƒë·ªß 100 events (~10-30 gi√¢y) |
| **Ph∆∞∆°ng th·ª©c** | `INSERT` batch v·ªõi `execute_values()` |
| **Data transformation** | `length_diff = length.new - length.old` ƒë∆∞·ª£c t√≠nh khi consume message |
| **Retention** | **24 gi·ªù** - Auto cleanup c√πng v·ªõi `realtime_traffic_volume` |

**üîÑ Flow ghi d·ªØ li·ªáu**:
```
Kafka (wiki-raw-events)
    ‚Üì [consume individual messages]
Quix Job Buffer:
    - Extract: event_time, user, title, server_name, is_bot, type, bytes_changed
    - Calculate: length_diff = length.new - length.old
    - Append to buffer (list of tuples)
    ‚Üì [when buffer >= 100 events]
Batch INSERT to PostgreSQL
    ‚Üì [execute_values()]
PostgreSQL: realtime_recent_changes
```

**üéØ M·ª•c ƒë√≠ch s·ª≠ d·ª•ng**:
- Dashboard: Recent changes feed (live activity log)
- User activity monitoring: Track specific users/bots
- Content analysis: Xem changes theo wiki/language
- Real-time alerts: Detect vandalism, spam

**üßπ Cleanup Policy**:
```sql
-- Auto-cleanup trong quix_job.py
DELETE FROM realtime_recent_changes 
WHERE event_time < NOW() - INTERVAL '24 HOURS';
```

---

### **B. Historical Tables (Batch Layer)**

C√°c b·∫£ng n√†y ƒë∆∞·ª£c c·∫≠p nh·∫≠t **ƒë·ªãnh k·ª≥** b·ªüi Spark Batch Processor, x·ª≠ l√Ω d·ªØ li·ªáu l·ªãch s·ª≠ t·ª´ S3 Data Lake.

---

#### 3. `historical_hourly_patterns`

**üìù M√¥ t·∫£**: Ph√¢n t√≠ch patterns theo gi·ªù trong ng√†y (0-23h), aggregated data cho historical analysis.

**üèóÔ∏è Schema** (Inferred t·ª´ Spark DataFrame):
```sql
-- Schema ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông b·ªüi Spark JDBC write
CREATE TABLE historical_hourly_patterns (
    hour_bucket TIMESTAMP,             -- C·ª≠a s·ªï gi·ªù (time-series)
    total_events BIGINT,               -- T·ªïng s·ªë events trong gi·ªù ƒë√≥
    total_bytes BIGINT,                -- T·ªïng bytes thay ƒë·ªïi
    avg_bytes DOUBLE PRECISION,        -- Trung b√¨nh bytes/event
    bot_events BIGINT,                 -- S·ªë events c·ªßa bots
    human_events BIGINT                -- S·ªë events c·ªßa humans
);
```

**üìä C·∫•u tr√∫c d·ªØ li·ªáu**:
| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `hour_bucket` | TIMESTAMP | Gi·ªù bucket | `2026-01-18 14:00:00` |
| `total_events` | BIGINT | Total events | `5234` |
| `total_bytes` | BIGINT | Total bytes | `2500000` |
| `avg_bytes` | DOUBLE | Avg bytes | `477.5` |
| `bot_events` | BIGINT | Bot events | `3200` |
| `human_events` | BIGINT | Human events | `2034` |

**‚úçÔ∏è C∆° ch·∫ø ghi d·ªØ li·ªáu**:

| Thu·ªôc t√≠nh | Chi ti·∫øt |
|------------|----------|
| **Component ghi** | `processing/batch_job.py` - WikiBatchProcessor |
| **V·ªã tr√≠ trong code** | [processing/batch_job.py](../processing/batch_job.py#L139-L152) - `_calculate_hourly_patterns()` |
| **Ngu·ªìn d·ªØ li·ªáu** | S3 Data Lake (`s3://wiki-data-lake-prod-1407/raw_events/`) |
| **Chu k·ª≥ ghi** | **ƒê·ªãnh k·ª≥** - Ch·∫°y theo schedule (m·∫∑c ƒë·ªãnh: daily ho·∫∑c on-demand) |
| **Th·ªùi gian x·ª≠ l√Ω** | 5-10 ph√∫t (t√πy volume data) |
| **Ph∆∞∆°ng th·ª©c** | Spark JDBC write v·ªõi `mode="overwrite"` - **thay th·∫ø to√†n b·ªô b·∫£ng** |
| **Data range** | M·∫∑c ƒë·ªãnh: 7 ng√†y g·∫ßn nh·∫•t (configurable qua `--days` parameter) |

**üîÑ Flow ghi d·ªØ li·ªáu**:
```
S3 Data Lake (Parquet files)
    ‚Üì [Spark read from s3a://]
PySpark DataFrame:
    - Filter: Last N days
    - date_trunc("hour", timestamp) as hour_bucket
    - GROUP BY hour_bucket
    - AGG: count(*), sum(bytes_changed), avg(bytes_changed)
    - COUNT bot vs human events
    ‚Üì [Spark JDBC Writer]
PostgreSQL: historical_hourly_patterns (OVERWRITE entire table)
```

**‚ö†Ô∏è Important Notes**:
- **OVERWRITE mode**: M·ªói l·∫ßn ch·∫°y s·∫Ω replace to√†n b·ªô data trong b·∫£ng
- **No incremental updates**: Kh√¥ng append, ch·ªâ full refresh
- **Historical timerange**: Ch·ªâ gi·ªØ data c·ªßa N days g·∫ßn nh·∫•t (default 7 days)

**üéØ M·ª•c ƒë√≠ch s·ª≠ d·ª•ng**:
- Dashboard: Hourly activity patterns chart
- Trend analysis: So s√°nh patterns theo ng√†y
- Bot vs Human ratio analysis

---

#### 4. `historical_hourly_trends`

**üìù M√¥ t·∫£**: Time-series hourly trends v·ªõi 24-hour moving average, cho ph√©p ph√¢n t√≠ch xu h∆∞·ªõng theo th·ªùi gian.

**üèóÔ∏è Schema**:
```sql
CREATE TABLE historical_hourly_trends (
    hour_bucket TIMESTAMP,             -- C·ª≠a s·ªï gi·ªù
    total_events BIGINT,               -- T·ªïng events
    total_bytes BIGINT,                -- T·ªïng bytes
    bot_count BIGINT,                  -- S·ªë bot events
    new_pages BIGINT,                  -- S·ªë trang m·ªõi t·∫°o
    edits BIGINT,                      -- S·ªë edits
    events_24h_avg DOUBLE PRECISION,   -- Moving avg events (24h window)
    bytes_24h_avg DOUBLE PRECISION     -- Moving avg bytes (24h window)
);
```

**üìä C·∫•u tr√∫c d·ªØ li·ªáu**:
| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `hour_bucket` | TIMESTAMP | Hour window | `2026-01-18 15:00:00` |
| `total_events` | BIGINT | Events count | `4521` |
| `total_bytes` | BIGINT | Bytes changed | `2100000` |
| `bot_count` | BIGINT | Bot edits | `2800` |
| `new_pages` | BIGINT | New pages | `45` |
| `edits` | BIGINT | Edit count | `4476` |
| `events_24h_avg` | DOUBLE | 24h MA events | `4350.2` |
| `bytes_24h_avg` | DOUBLE | 24h MA bytes | `2050000.5` |

**‚úçÔ∏è C∆° ch·∫ø ghi d·ªØ li·ªáu**:

| Thu·ªôc t√≠nh | Chi ti·∫øt |
|------------|----------|
| **Component ghi** | `processing/batch_job.py` - WikiBatchProcessor |
| **V·ªã tr√≠ trong code** | [processing/batch_job.py](../processing/batch_job.py#L183-L215) - `_calculate_hourly_trends()` |
| **Ngu·ªìn d·ªØ li·ªáu** | S3 Data Lake (Parquet) |
| **Chu k·ª≥ ghi** | **ƒê·ªãnh k·ª≥** - Schedule based (default: daily) |
| **Ph∆∞∆°ng th·ª©c** | Spark JDBC write v·ªõi `mode="overwrite"` |
| **Special processing** | S·ª≠ d·ª•ng Spark Window function cho moving average |

**üîÑ Flow ghi d·ªØ li·ªáu**:
```
S3 Data Lake
    ‚Üì [Spark read]
PySpark DataFrame:
    - date_trunc("hour", timestamp) as hour_bucket
    - GROUP BY hour_bucket: count, sum, filters
    - Window function: 
        Window.orderBy("hour_bucket").rowsBetween(-23, 0)
        => Calculate 24-hour moving averages
    - ORDER BY hour_bucket
    ‚Üì [Spark JDBC]
PostgreSQL: historical_hourly_trends (OVERWRITE)
```

**üéØ M·ª•c ƒë√≠ch s·ª≠ d·ª•ng**:
- Dashboard: Trend analysis charts v·ªõi smoothing
- Anomaly detection: Detect spikes vs 24h average
- Forecasting: Predict future traffic based on trends

---

#### 5. `historical_top_contributors`

**üìù M√¥ t·∫£**: Top 100 contributors (users/bots) ƒë∆∞·ª£c x·∫øp h·∫°ng theo s·ªë l∆∞·ª£ng edits.

**üèóÔ∏è Schema**:
```sql
CREATE TABLE historical_top_contributors (
    "user" TEXT,                       -- Username
    is_bot BOOLEAN,                    -- Bot flag
    edit_count BIGINT,                 -- Total edits
    total_bytes BIGINT,                -- Total bytes changed
    pages_created BIGINT               -- Pages created by user
);
```

**üìä C·∫•u tr√∫c d·ªØ li·ªáu**:
| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `"user"` | TEXT | Username | `InternetArchiveBot` |
| `is_bot` | BOOLEAN | Bot flag | `true` |
| `edit_count` | BIGINT | Edit count | `125000` |
| `total_bytes` | BIGINT | Bytes changed | `50000000` |
| `pages_created` | BIGINT | Pages created | `250` |

**‚úçÔ∏è C∆° ch·∫ø ghi d·ªØ li·ªáu**:

| Thu·ªôc t√≠nh | Chi ti·∫øt |
|------------|----------|
| **Component ghi** | `processing/batch_job.py` - WikiBatchProcessor |
| **V·ªã tr√≠ trong code** | [processing/batch_job.py](../processing/batch_job.py#L154-L168) - `_calculate_top_contributors()` |
| **Ngu·ªìn d·ªØ li·ªáu** | S3 Data Lake |
| **Chu k·ª≥ ghi** | **ƒê·ªãnh k·ª≥** - Schedule based |
| **Limit** | Top 100 contributors only |
| **Ph∆∞∆°ng th·ª©c** | Spark JDBC `mode="overwrite"` |

**üîÑ Flow ghi d·ªØ li·ªáu**:
```
S3 Data Lake
    ‚Üì [Spark read]
PySpark DataFrame:
    - GROUP BY user, is_bot
    - AGG: 
        count(*) as edit_count
        sum(bytes_changed) as total_bytes
        count(is_new_page == true) as pages_created
    - ORDER BY edit_count DESC
    - LIMIT 100
    ‚Üì [Spark JDBC]
PostgreSQL: historical_top_contributors (OVERWRITE, max 100 rows)
```

**üéØ M·ª•c ƒë√≠ch s·ª≠ d·ª•ng**:
- Dashboard: Top contributors leaderboard
- Community analysis: Identify key contributors
- Bot vs Human comparison

---

#### 6. `historical_language_distribution`

**üìù M√¥ t·∫£**: Ph√¢n ph·ªëi edits theo ng√¥n ng·ªØ v√† project (wikipedia, wiktionary, etc.).

**üèóÔ∏è Schema**:
```sql
CREATE TABLE historical_language_distribution (
    language TEXT,                     -- Language code (en, es, fr, etc.)
    project TEXT,                      -- Project type (wikipedia, wiktionary)
    edit_count BIGINT,                 -- Total edits
    total_bytes BIGINT,                -- Total bytes
    bot_edits BIGINT,                  -- Bot edits count
    new_pages BIGINT                   -- New pages created
);
```

**üìä C·∫•u tr√∫c d·ªØ li·ªáu**:
| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `language` | TEXT | Language code | `en` |
| `project` | TEXT | Project name | `wikipedia` |
| `edit_count` | BIGINT | Edit count | `2500000` |
| `total_bytes` | BIGINT | Bytes changed | `1200000000` |
| `bot_edits` | BIGINT | Bot edits | `1500000` |
| `new_pages` | BIGINT | New pages | `8500` |

**‚úçÔ∏è C∆° ch·∫ø ghi d·ªØ li·ªáu**:

| Thu·ªôc t√≠nh | Chi ti·∫øt |
|------------|----------|
| **Component ghi** | `processing/batch_job.py` - WikiBatchProcessor |
| **V·ªã tr√≠ trong code** | [processing/batch_job.py](../processing/batch_job.py#L170-L184) - `_calculate_language_distribution()` |
| **Ngu·ªìn d·ªØ li·ªáu** | S3 Data Lake |
| **Chu k·ª≥ ghi** | **ƒê·ªãnh k·ª≥** - Schedule based |
| **Ph∆∞∆°ng th·ª©c** | Spark JDBC `mode="overwrite"` |

**üîÑ Flow ghi d·ªØ li·ªáu**:
```
S3 Data Lake
    ‚Üì [Spark read]
PySpark DataFrame:
    - GROUP BY language, project
    - AGG:
        count(*) as edit_count
        sum(bytes_changed) as total_bytes
        count(is_bot == true) as bot_edits
        count(is_new_page == true) as new_pages
    - ORDER BY edit_count DESC
    ‚Üì [Spark JDBC]
PostgreSQL: historical_language_distribution (OVERWRITE)
```

**üéØ M·ª•c ƒë√≠ch s·ª≠ d·ª•ng**:
- Dashboard: Language/project distribution pie chart
- Geo analysis: Wikipedia popularity by region
- Multi-lingual insights

---

#### 7. `historical_server_rankings`

**üìù M√¥ t·∫£**: Top 50 wiki servers ƒë∆∞·ª£c x·∫øp h·∫°ng theo activity, bao g·ªìm percentile rankings.

**üèóÔ∏è Schema**:
```sql
CREATE TABLE historical_server_rankings (
    server_name TEXT,                  -- Server domain (en.wikipedia.org)
    language TEXT,                     -- Language code
    project TEXT,                      -- Project type
    edit_count BIGINT,                 -- Total edits
    total_bytes BIGINT,                -- Total bytes
    unique_users BIGINT,               -- Distinct users count
    rank INT,                          -- Ranking position (1-50)
    percentile DOUBLE PRECISION        -- Percentile rank (0.0-1.0)
);
```

**üìä C·∫•u tr√∫c d·ªØ li·ªáu**:
| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `server_name` | TEXT | Server domain | `en.wikipedia.org` |
| `language` | TEXT | Language | `en` |
| `project` | TEXT | Project | `wikipedia` |
| `edit_count` | BIGINT | Edit count | `3000000` |
| `total_bytes` | BIGINT | Bytes changed | `1500000000` |
| `unique_users` | BIGINT | Unique users | `25000` |
| `rank` | INT | Rank position | `1` |
| `percentile` | DOUBLE | Percentile | `1.0` (top) |

**‚úçÔ∏è C∆° ch·∫ø ghi d·ªØ li·ªáu**:

| Thu·ªôc t√≠nh | Chi ti·∫øt |
|------------|----------|
| **Component ghi** | `processing/batch_job.py` - WikiBatchProcessor |
| **V·ªã tr√≠ trong code** | [processing/batch_job.py](../processing/batch_job.py#L217-L242) - `_calculate_server_rankings()` |
| **Ngu·ªìn d·ªØ li·ªáu** | S3 Data Lake |
| **Chu k·ª≥ ghi** | **ƒê·ªãnh k·ª≥** - Schedule based |
| **Limit** | Top 50 servers only |
| **Ph∆∞∆°ng th·ª©c** | Spark JDBC `mode="overwrite"` |
| **Special processing** | S·ª≠ d·ª•ng Spark Window functions: `row_number()`, `percent_rank()` |

**üîÑ Flow ghi d·ªØ li·ªáu**:
```
S3 Data Lake
    ‚Üì [Spark read]
PySpark DataFrame:
    - GROUP BY server_name, language, project
    - AGG:
        count(*) as edit_count
        sum(bytes_changed) as total_bytes
        countDistinct(user) as unique_users
    - Window function:
        Window.orderBy(desc(edit_count))
        => row_number() as rank
        => percent_rank() as percentile
    - FILTER: rank <= 50
    ‚Üì [Spark JDBC]
PostgreSQL: historical_server_rankings (OVERWRITE, max 50 rows)
```

**üéØ M·ª•c ƒë√≠ch s·ª≠ d·ª•ng**:
- Dashboard: Server rankings leaderboard
- Comparative analysis: Wiki activity comparison
- Community size analysis via unique_users

---

## üóÇÔ∏è S3 Data Lake Structure

### Raw Events Storage

**Path structure**:
```
s3://wiki-data-lake-prod-1407/
‚îî‚îÄ‚îÄ raw_events/
    ‚îú‚îÄ‚îÄ year=2026/
    ‚îÇ   ‚îú‚îÄ‚îÄ month=01/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wiki_events_20260118_140530.parquet
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wiki_events_20260118_141645.parquet
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (multiple parquet files)
    ‚îÇ   ‚îî‚îÄ‚îÄ month=02/
    ‚îî‚îÄ‚îÄ year=2025/
        ‚îî‚îÄ‚îÄ ...
```

**File Format**: Apache Parquet
- **Compression**: Snappy
- **Version**: 2.6
- **Schema**: All fields from Kafka events + enrichments

**Partitioning Strategy**:
- **By time**: `year=YYYY/month=MM/`
- **Purpose**: Optimize Spark queries by date range
- **Benefits**: Partition pruning reduces scan volume

**‚úçÔ∏è C∆° ch·∫ø ghi d·ªØ li·ªáu**:

| Thu·ªôc t√≠nh | Chi ti·∫øt |
|------------|----------|
| **Component ghi** | `ingestion/batch_job.py` - Batch Archiver |
| **V·ªã tr√≠ trong code** | [ingestion/batch_job.py](../ingestion/batch_job.py#L63-L106) - `upload_batch()` |
| **Ngu·ªìn d·ªØ li·ªáu** | Kafka topic `wiki-raw-events` |
| **Chu k·ª≥ ghi** | **Micro-batch**: M·ªói 5000 events HO·∫∂C m·ªói 60 gi√¢y |
| **Ph∆∞∆°ng th·ª©c** | PyArrow Parquet writer ‚Üí boto3 S3 upload |
| **File size** | ~2-5 MB per file (t√πy traffic) |
| **Write rate** | ~1-3 files/ph√∫t (peak: 5-10 files/ph√∫t) |

**üîÑ Flow ghi d·ªØ li·ªáu**:
```
Kafka (wiki-raw-events)
    ‚Üì [consumer: wiki-batch-archiver-group]
In-memory Buffer (list)
    ‚Üì [buffer until 5000 events OR 60s timeout]
Data Normalization:
    - timestamp ‚Üí int64
    - log_params ‚Üí JSON string
    ‚Üì [pandas DataFrame]
PyArrow Table Conversion
    ‚Üì [write_table() to BytesIO buffer]
boto3 S3 Client
    ‚Üì [upload_fileobj()]
S3: s3://wiki-data-lake-prod-1407/raw_events/year=*/month=*/*.parquet
```

**üì¶ File Example**:
```
Filename: wiki_events_20260118_140530.parquet
Size: 3.2 MB
Records: 5000 events
Columns: 
  - timestamp (int64)
  - user (string)
  - title (string)
  - server_name (string)
  - type (string)
  - bytes_changed (int64)
  - is_bot (bool)
  - is_new_page (bool)
  - language (string)
  - project (string)
  - log_params (string, JSON)
  - ... (20+ fields total)
```

**üéØ M·ª•c ƒë√≠ch s·ª≠ d·ª•ng**:
- **Primary storage**: Long-term archive c·ªßa raw events
- **Batch processing**: Source data cho Spark jobs
- **Reprocessing**: C√≥ th·ªÉ reprocess l·∫°i data c≈© n·∫øu c·∫ßn
- **Data auditing**: Full historical record

---

## üìä Summary Table - All Tables & Data Sources

| # | Table Name | Database | Type | Writer Component | Source Data | Write Cycle | Retention |
|---|-----------|----------|------|------------------|-------------|-------------|-----------|
| 1 | `realtime_traffic_volume` | PostgreSQL | Real-time | `quix_job.py` | Kafka | ~10-30s (100 events) | 24 hours |
| 2 | `realtime_recent_changes` | PostgreSQL | Real-time | `quix_job.py` | Kafka | ~10-30s (100 events) | 24 hours |
| 3 | `historical_hourly_patterns` | PostgreSQL | Historical | `batch_job.py` (Spark) | S3 Data Lake | Daily/scheduled | Last N days |
| 4 | `historical_hourly_trends` | PostgreSQL | Historical | `batch_job.py` (Spark) | S3 Data Lake | Daily/scheduled | Last N days |
| 5 | `historical_top_contributors` | PostgreSQL | Historical | `batch_job.py` (Spark) | S3 Data Lake | Daily/scheduled | Last N days |
| 6 | `historical_language_distribution` | PostgreSQL | Historical | `batch_job.py` (Spark) | S3 Data Lake | Daily/scheduled | Last N days |
| 7 | `historical_server_rankings` | PostgreSQL | Historical | `batch_job.py` (Spark) | S3 Data Lake | Daily/scheduled | Last N days |
| 8 | **S3 Raw Events** (Parquet) | S3 Data Lake | Archive | `batch_job.py` (Archiver) | Kafka | 60s or 5000 events | Indefinite |

---

## üîÑ Data Flow Summary

### 1. **Real-time Path (Speed Layer)**
```
Wikipedia EventStream
    ‚Üì
Producer (ingestion/producer.py)
    ‚Üì [filter, enrich, publish]
Kafka/Redpanda (wiki-raw-events topic)
    ‚Üì
Quix Stream Processor (processing/quix_job.py)
    ‚Üì [micro-batch aggregation]
PostgreSQL RDS:
    - realtime_traffic_volume
    - realtime_recent_changes
    ‚Üì
Streamlit Dashboard (dashboard/app.py)
```

**‚è±Ô∏è Latency**: <30 seconds end-to-end

---

### 2. **Batch Path (Batch Layer)**
```
Kafka (wiki-raw-events topic)
    ‚Üì
Batch Archiver (ingestion/batch_job.py)
    ‚Üì [buffer 5000 events or 60s]
S3 Data Lake (Parquet files, partitioned by year/month)
    ‚Üì
Spark Batch Processor (processing/batch_job.py)
    ‚Üì [complex aggregations, 7 days window]
PostgreSQL RDS:
    - historical_hourly_patterns
    - historical_hourly_trends
    - historical_top_contributors
    - historical_language_distribution
    - historical_server_rankings
    ‚Üì
Streamlit Dashboard (pages/1_Historical_Analytics.py)
```

**‚è±Ô∏è Latency**: 
- Archive: 1-2 minutes (micro-batch delay)
- Processing: 5-10 minutes (Spark job runtime)
- Total: ~10-15 minutes t·ª´ event ‚Üí historical tables

---

## üõ†Ô∏è Database Initialization

**Script**: [scripts/create_tables_rds.py](../scripts/create_tables_rds.py)

**Purpose**: T·∫°o 2 real-time tables v√† indexes

**Usage**:
```bash
python scripts/create_tables_rds.py
```

**Tables created**:
- ‚úÖ `realtime_traffic_volume` (with index on window_start)
- ‚úÖ `realtime_recent_changes` (with index on event_time)

**‚ö†Ô∏è Note**: 
- Historical tables ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông b·ªüi Spark khi ch·∫°y l·∫ßn ƒë·∫ßu (Spark JDBC auto-create)
- N·∫øu c·∫ßn recreate, c√≥ th·ªÉ DROP tables v√† ch·∫°y l·∫°i batch job

---

## üìà Performance Considerations

### Real-time Tables
- **Write throughput**: ~100-500 inserts/minute (batched)
- **Read queries**: Indexed on time columns ‚Üí Fast range queries
- **Auto-cleanup**: Gi·ªØ data 24h ‚Üí Prevent unlimited growth
- **Size estimate**: ~10-50 MB per table (steady state)

### Historical Tables
- **Write method**: OVERWRITE entire table (not incremental)
- **Write frequency**: Daily or on-demand
- **Size estimate**: 
  - Patterns/trends: ~1-5 MB
  - Contributors/servers: <1 MB (limited rows)
  - Language distribution: ~500 KB
- **No indexes**: Analytical queries, full table scans acceptable

### S3 Data Lake
- **Storage**: Grows indefinitely (~10-50 GB/month estimate)
- **Cost optimization**: Use S3 lifecycle policies (move to Glacier after 90 days)
- **Query performance**: Parquet columnar format + partitioning ‚Üí Efficient Spark reads

---

## üîê Access Configuration

### PostgreSQL Connection
```python
# From config/settings.py
POSTGRES_HOST = "wiki-pipeline-db.cen6acgi49ye.us-east-1.rds.amazonaws.com"
POSTGRES_PORT = 5432
POSTGRES_DB = "wikidb"
POSTGRES_USER = "thanh123"
POSTGRES_PASSWORD = "<from env or thongtin.txt>"
```

### S3 Access
```python
# From config/settings.py
S3_BUCKET = "wiki-data-lake-prod-1407"
S3_REGION = "us-east-1"

# Authentication:
# 1. EC2 Instance Profile (preferred): admin_thanh123
# 2. AWS credentials from environment
```

---

## üìö Related Documentation

- **Streaming Layer**: [STREAMING.md](STREAMING.md) - Chi ti·∫øt v·ªÅ real-time processing
- **Batch Layer**: [BATCH.md](BATCH.md) - Chi ti·∫øt v·ªÅ batch processing & Spark
- **Dashboard**: [DASHBOARD_REPORT.md](DASHBOARD_REPORT.md) - C√°ch dashboard query c√°c tables
- **System Architecture**: [NEW_SYSTEM_REPORT.md](NEW_SYSTEM_REPORT.md) - Ki·∫øn tr√∫c t·ªïng quan

---

## ‚úÖ Checklist - Table Creation

### Initial Setup (One-time)
- [ ] Run `create_tables_rds.py` ‚Üí Creates realtime tables
- [ ] Run batch archiver ‚Üí Starts writing to S3
- [ ] Run batch processor ‚Üí Creates historical tables (auto-create)
- [ ] Verify all 7 tables exist in PostgreSQL

### Verification Queries
```sql
-- List all tables
SELECT table_name FROM information_schema.tables 
WHERE table_schema = 'public';

-- Check row counts
SELECT COUNT(*) FROM realtime_traffic_volume;
SELECT COUNT(*) FROM realtime_recent_changes;
SELECT COUNT(*) FROM historical_hourly_patterns;
SELECT COUNT(*) FROM historical_hourly_trends;
SELECT COUNT(*) FROM historical_top_contributors;
SELECT COUNT(*) FROM historical_language_distribution;
SELECT COUNT(*) FROM historical_server_rankings;

-- Check indexes
SELECT tablename, indexname FROM pg_indexes 
WHERE schemaname = 'public';
```

---

**Document Version**: 1.0  
**Last Updated**: 2026-01-19  
**Author**: System Documentation  
**Status**: ‚úÖ Complete
