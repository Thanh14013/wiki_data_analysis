# Wiki Data Analysis System - Complete Technical Report

**Version**: 2.0 (Production)  
**Date**: January 15, 2026  
**Author**: thanh123  
**Architecture**: Lambda Architecture (Speed + Batch Layers)

---

## üìã Executive Summary

**Wiki Data Analysis** l√† m·ªôt h·ªá th·ªëng x·ª≠ l√Ω d·ªØ li·ªáu real-time v√† batch processing theo **Lambda Architecture**, thu th·∫≠p v√† ph√¢n t√≠ch h√†ng tri·ªáu s·ª± ki·ªán ch·ªânh s·ª≠a t·ª´ Wikipedia to√†n c·∫ßu. H·ªá th·ªëng cung c·∫•p insights t·ª©c th√¨ (<15 gi√¢y latency) v√† deep analytics cho historical data.

### Key Features:
- ‚úÖ **Real-time Processing**: X·ª≠ l√Ω 500-1000 events/gi√¢y v·ªõi latency <15s
- ‚úÖ **Batch Analytics**: Historical insights t·ª´ Data Lake (S3) qua Apache Spark
- ‚úÖ **Scalable**: 3 processor replicas v·ªõi consumer group balancing
- ‚úÖ **Fault Tolerant**: At-least-once semantics, auto-recovery
- ‚úÖ **Cloud Native**: Deploy tr√™n AWS (EC2 + RDS + S3)
- ‚úÖ **Cost Effective**: ~$44/th√°ng cho production workload

---

## üèóÔ∏è Architecture Overview

H·ªá th·ªëng ƒë∆∞·ª£c thi·∫øt k·∫ø theo **Lambda Architecture** pattern v·ªõi 2 layers song song:

### Lambda Architecture Diagram

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ         WIKIMEDIA EVENTSTREAM API               ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚îÇ
                                     ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ         PRODUCER (Filter & Enrich)              ‚îÇ
                    ‚îÇ  - Filter noise (talk pages, categorize)        ‚îÇ
                    ‚îÇ  - Add analytics fields (is_bot, bytes_changed) ‚îÇ
                    ‚îÇ  - Partitioning by server_name                  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ           ‚îÇ
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ                                        ‚îÇ
                   ‚ñº                                        ‚ñº
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó          ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë      SPEED LAYER              ‚ïë          ‚ïë      BATCH LAYER             ‚ïë
    ‚ïë   (Real-time, Low Latency)   ‚ïë          ‚ïë (Historical, High Accuracy)  ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù          ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
                   ‚îÇ                                        ‚îÇ
                   ‚ñº                                        ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   KAFKA/REDPANDA             ‚îÇ          ‚îÇ   BATCH INGESTION            ‚îÇ
    ‚îÇ   - Topic: wiki-raw-events   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   - Buffer 5000 events       ‚îÇ
    ‚îÇ   - 3 partitions             ‚îÇ          ‚îÇ   - Write every 60s          ‚îÇ
    ‚îÇ   - 24h retention            ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
               ‚îÇ                                          ‚ñº
               ‚ñº                               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ   S3 DATA LAKE               ‚îÇ
    ‚îÇ   QUIX PROCESSOR (3 replicas)‚îÇ          ‚îÇ   - Parquet format           ‚îÇ
    ‚îÇ   - Micro-batch (100 events) ‚îÇ          ‚îÇ   - Partitioned: year/month  ‚îÇ
    ‚îÇ   - Aggregations             ‚îÇ          ‚îÇ   - Compressed (Snappy)      ‚îÇ
    ‚îÇ   - Consumer group           ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
               ‚îÇ                                          ‚ñº
               ‚ñº                               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ   SPARK BATCH PROCESSOR      ‚îÇ
    ‚îÇ   POSTGRESQL (RDS)            ‚îÇ          ‚îÇ   - Read from S3             ‚îÇ
    ‚îÇ   REALTIME TABLES:            ‚îÇ          ‚îÇ   - Complex aggregations     ‚îÇ
    ‚îÇ   - traffic_volume            ‚îÇ          ‚îÇ   - 5 analytics views        ‚îÇ
    ‚îÇ   - recent_changes            ‚îÇ          ‚îÇ   - Hourly schedule          ‚îÇ
    ‚îÇ   - 24h retention             ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
               ‚îÇ                                          ‚ñº
               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   POSTGRESQL (RDS) - HISTORICAL TABLES          ‚îÇ
                    ‚îÇ   - historical_hourly_patterns                  ‚îÇ
                    ‚îÇ   - historical_top_contributors                 ‚îÇ
                    ‚îÇ   - historical_language_distribution            ‚îÇ
                    ‚îÇ   - historical_server_rankings                  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ         STREAMLIT DASHBOARD                      ‚îÇ
                    ‚îÇ   - Real-time Analytics (Speed Layer)           ‚îÇ
                    ‚îÇ   - Historical Analytics (Batch Layer)          ‚îÇ
                    ‚îÇ   - Auto-refresh every 8s                       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Architecture Highlights

**Speed Layer (Real-time):**
- **Latency**: <15 seconds end-to-end
- **Throughput**: 500-1000 events/s per replica
- **Purpose**: Dashboard real-time metrics (30-minute window)
- **Trade-off**: Approximate accuracy, limited history

**Batch Layer (Historical):**
- **Latency**: ~1 hour (configurable)
- **Throughput**: Process 100K+ events in 2-3 minutes
- **Purpose**: Deep analytics, trends, rankings
- **Trade-off**: Higher latency, perfect accuracy

**Serving Layer:**
- **Database**: Single PostgreSQL instance v·ªõi separate tables
- **Queries**: Optimized v·ªõi indexes cho fast dashboard rendering
- **Retention**: Real-time (24h), Historical (unlimited)

---

## üèóÔ∏è Components Deep Dive

### 1. Data Ingestion

### 1. Data Ingestion

#### 1.1 Wikimedia Producer (`ingestion/producer.py`)

**Purpose**: K·∫øt n·ªëi t·ªõi Wikipedia EventStream API, filter v√† enrich events, publish v√†o Kafka.

**Technical Implementation:**
```python
Protocol: Server-Sent Events (SSE)
URL: https://stream.wikimedia.org/v2/stream/recentchange
Rate: 50-500 events/second (varies by time of day)
Format: JSON over HTTP/1.1 keep-alive
```

**Filtering Logic:**
- ‚ùå Skip: talk pages, user_talk, wikipedia_talk
- ‚ùå Skip: type != edit/new/log
- ‚úÖ Keep: Main article edits, new pages, log events
- **Result**: ~40% reduction in noise

**Enrichment:**
- `is_bot`: Boolean flag t·ª´ event.bot
- `is_new_page`: Detect page creation
- `bytes_changed`: abs(length.new - length.old)
- `language`, `project`: Parse t·ª´ server_name (e.g., en.wikipedia.org)

**Kafka Publishing:**
```python
Config:
- acks='all': Wait for all replicas
- retries=3: Auto-retry failed sends
- compression='gzip': 60-70% bandwidth savings
- key=server_name: Partition by wiki server

Performance:
- Throughput: 200-500 events/s
- Latency: <50ms per event
- Success rate: >99.5%
```

#### 1.2 Kafka/Redpanda (Message Broker)

**Role**: Decoupling v√† buffering gi·ªØa producer v√† consumers.

**Configuration:**
```yaml
Type: Single-node Redpanda (Kafka-compatible)
Memory: 1GB
CPU: 1 core
Topic: wiki-raw-events
Partitions: 3 (auto-assigned)
Retention: 24 hours
Replication: 1 (single node)
```

**Why Redpanda vs Kafka?**
- ‚úÖ Lighter resource footprint
- ‚úÖ No JVM dependencies
- ‚úÖ Faster startup time
- ‚úÖ 100% Kafka API compatible

**Scaling Strategy:**
```
Development: 1 node (current)
‚Üí Production: 3 nodes cluster
  - Replication factor: 3
  - Min in-sync replicas: 2
  - Partitions: 6-12 (2x processors)
```

---

### 2. Speed Layer (Real-time Processing)

#### 2.1 Quix Stream Processor (`processing/quix_job.py`)

**Architecture:**
```
Deployment: 3 Docker container replicas
Consumer Group: wiki-quix-group
Processing Model: Micro-batching (100 events/batch)
Write Strategy: Bulk INSERT to PostgreSQL
```

**Micro-batching Flow:**
```python
1. Poll events from Kafka (consumer.poll)
2. Buffer in-memory list
3. When buffer reaches 100 events:
   a. Aggregate metrics
   b. Bulk INSERT to PostgreSQL (execute_values)
   c. Commit Kafka offset
4. Repeat
```

**Why Micro-batching?**
- üöÄ 100x fewer database connections
- üöÄ 10-20x throughput increase
- üöÄ Lower average latency (despite batching)
- ‚öñÔ∏è Trade-off: Max 10s delay for dashboard

**Real-time Tables:**

**`realtime_traffic_volume`:**
```sql
Columns: window_start, total_bytes, event_count, avg_bytes_per_event
Purpose: Traffic trends over time
Update: Every ~10 seconds (per micro-batch)
Retention: 24 hours (auto-cleanup)
```

**`realtime_recent_changes`:**
```sql
Columns: event_time, user, title, server_name, is_bot, type, bytes_changed, length_diff
Purpose: Raw feed cho dashboard queries
Update: Continuous stream
Retention: 24 hours
Indexes: event_time, server_name, user
```

**Performance Metrics:**
```
Per Replica:
- CPU: 10-20%
- Memory: 100-200MB
- Throughput: 500-1000 events/s
- Latency: <10s (micro-batch window)

Aggregate (3 replicas):
- Total throughput: 1500-3000 events/s
- Load balanced via consumer group
- Fault tolerant (2/3 can handle load)
```

**Fault Tolerance:**
```
Scenario: 1 replica crashes
‚Üí Consumer group rebalancing (30s)
‚Üí Remaining 2 replicas take over partitions
‚Üí No data loss (offset not committed until write succeeds)
‚Üí At-least-once processing semantics
```

---

### 3. Batch Layer (Historical Analytics)

#### 3.1 Batch Ingestion (`ingestion/batch_job.py`)

**Purpose**: Archive raw events t·ª´ Kafka v√†o S3 Data Lake.

**Buffering Strategy:**
```python
BATCH_SIZE = 5000 events
TIMEOUT = 60 seconds

Flush triggers (OR condition):
- Buffer reaches 5000 events
- 60 seconds since last flush

Output: Parquet files in S3
```

**S3 Storage Structure:**
```
s3://wiki-data-lake-prod-1407/raw_data/raw_events/
‚îú‚îÄ‚îÄ year=2026/
‚îÇ   ‚îú‚îÄ‚îÄ month=01/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wiki_events_20260115_070916.parquet  (5000 events, ~2MB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wiki_events_20260115_071017.parquet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (1 file per minute average)
‚îÇ   ‚îî‚îÄ‚îÄ month=02/
‚îî‚îÄ‚îÄ year=2027/
```

**Partitioning Benefits:**
- Hive-compatible partitioning
- Spark partition pruning (only read needed months)
- Easy data lifecycle management
- Query optimization

**Data Format:**
```
Format: Apache Parquet
Compression: Snappy (5-10x compression)
Schema: Auto-inferred from DataFrame
Encoding: Dictionary encoding for strings
Version: 2.6 (Spark 3.x compatible)
```

**Performance:**
```
Write frequency: 1 file/minute (peak), 1 file/5min (average)
File size: 500KB - 5MB compressed
Throughput: ~83 events/s sustained
S3 write latency: 500ms - 2s
```

#### 3.2 Spark Batch Processor (`processing/batch_job.py`)

**Purpose**: ƒê·ªçc data t·ª´ S3, ch·∫°y complex analytics, ghi v√†o PostgreSQL historical tables.

**Spark Configuration:**
```python
Version: PySpark 3.5.1
Master: local[*] (use all cores)
Memory: 1-2GB driver, auto executors
Shuffle partitions: 200 (configurable)

Optimizations:
- Adaptive Query Execution: Enabled
- Dynamic partition coalescing: Enabled
- S3A FileSystem: Optimized reads
```

**Data Processing Flow:**
```
1. Read Parquet from S3 (wildcard: year=*/month=*/*)
2. Convert timestamp (int64 ‚Üí TimestampType)
3. Filter by date range (default: last 1 day)
4. Run 5 analytics computations
5. Write results to PostgreSQL (mode=overwrite)
```

**Analytics Computed:**

**1. Hourly Patterns:**
```sql
Aggregation: GROUP BY date_trunc('hour', timestamp)
Metrics: total_events, total_bytes, avg_bytes, bot_events, human_events
Output: Time-series data (24-168 rows)
```

**2. Hourly Trends:**
```sql
Aggregation: Hourly + 24-hour moving average
Window: rowsBetween(-23, 0)
Metrics: events_24h_avg, bytes_24h_avg
Purpose: Smoothed trends for analysis
```

**3. Top Contributors:**
```sql
Aggregation: GROUP BY user, is_bot
Metrics: edit_count, total_bytes, pages_created
Output: Top 100 contributors
```

**4. Language Distribution:**
```sql
Aggregation: GROUP BY language, project
Metrics: edit_count, total_bytes, bot_edits, new_pages
Output: ~50-200 language-project pairs
```

**5. Server Rankings:**
```sql
Aggregation: GROUP BY server_name
Metrics: edit_count, total_bytes, unique_users
Window: rank(), percent_rank()
Output: Top 50 servers with rankings
```

**Execution Schedule:**
```bash
Current: Manual or cron
Frequency: Hourly or daily
Command: python processing/batch_job.py --days=1

Future: Apache Airflow DAG
- Automated scheduling
- Dependency management
- Retry logic
- SLA monitoring
```

**Performance:**
```
Processing time:
- 10K events: ~30 seconds
- 100K events: ~2-3 minutes
- 1M events: ~15-20 minutes

Resource usage:
- CPU: 50-80% during processing
- Memory: 1-2GB peak
- Network: S3 read bandwidth limited
```

---

### 4. Serving Layer (Database)

#### 4.1 PostgreSQL on AWS RDS

**Instance Configuration:**
```
Type: db.t3.micro (Free Tier eligible)
vCPU: 2
Memory: 1 GB
Storage: 20 GB SSD (gp2)
Multi-AZ: No (can enable for HA)
Backup: Automated daily
```

**Schema Design:**

**Real-time Tables** (Speed Layer):
```sql
realtime_traffic_volume:
  - window_start (PK, indexed)
  - total_bytes, event_count, avg_bytes_per_event
  - Retention: 24 hours
  - Row count: ~200-300

realtime_recent_changes:
  - id (PK), event_time (indexed), user, title, server_name
  - is_bot, type, bytes_changed, length_diff
  - Retention: 24 hours
  - Row count: 50K-100K
  - Indexes: event_time DESC, server_name, user
```

**Historical Tables** (Batch Layer):
```sql
historical_hourly_patterns:
  - hour_bucket (PK), total_events, total_bytes, ...
  - Updated: Hourly by Spark
  - Row count: 24-168 (1-7 days)

historical_hourly_trends:
  - hour_bucket (PK), total_events, events_24h_avg, ...
  - Row count: 24-168

historical_top_contributors:
  - user (PK), is_bot, edit_count, total_bytes, pages_created
  - Row count: 100

historical_language_distribution:
  - language, project (composite PK), edit_count, total_bytes
  - Row count: 50-200

historical_server_rankings:
  - server_name (PK), rank, percentile, edit_count, unique_users
  - Row count: 50
```

**Query Performance:**
```
Real-time queries: <100ms (indexed)
Aggregations: <300ms
Dashboard refresh: ~500ms total (multiple queries)
```

---

### 5. Presentation Layer (Dashboard)

#### 5.1 Streamlit Dashboard (`dashboard/app.py`)

**Architecture:**
```python
Framework: Streamlit 1.30+
Server: Built-in Tornado server
Port: 8501
Deployment: Docker container
```

**Dashboard Features:**

**Main Page (Real-time Analytics):**
- KPI Cards: Events, Volume, Velocity, Top Server
- Traffic Volume: Line/area chart (30-min window)
- Top Servers: Bar chart (Top 12)
- Language Distribution: Bar chart with color scale
- Live Edits Scatter: The Battlefield visualization
- Edit Severity: Pie chart (Major/Moderate/Minor)
- User Engagement: Histogram
- Namespace Distribution: Bar chart
- Auto-refresh: Every 8 seconds

**Historical Analytics Page:**
- Hourly Trends: Line chart v·ªõi 24h moving average
- Top Contributors: Leaderboard table
- Language Stats: Interactive table
- Server Rankings: Ranked table v·ªõi percentile
- Refresh: Manual or configurable

**Query Strategy:**
```python
Lookback window: 30 minutes (real-time)
Batch size: 400 rows (recent changes limit)
Caching: None (always fresh data)
Connection: Direct to PostgreSQL RDS
```

**Performance:**
```
Page load: <2 seconds
Query execution: ~500ms
Auto-refresh overhead: Minimal (efficient queries)
Concurrent users: 10-20 (limited by RDS)
```

---

## üìä System Performance & Metrics

### Current Capacity

**Speed Layer:**
```
Throughput: 1500-3000 events/s (3 replicas combined)
Latency: <15 seconds end-to-end
Database writes: 15-30 batches/second
CPU usage: 30-40% average
Memory: 300-600MB total
```

**Batch Layer:**
```
Ingestion: 5000 events/60s = 83 events/s to S3
Processing: 100K events in 2-3 minutes
S3 storage: 10GB/month @ 100M events
Spark jobs: 1-2 hours total daily
```

**Bottlenecks:**
```
1. Producer: Limited by Wikipedia stream rate (~500 events/s peak)
2. RDS connections: Free tier limited to 20 connections
3. S3 bandwidth: Negligible for current scale
4. Dashboard queries: Acceptable (<500ms)
```

### Scaling Projections

**10x Scale (5000 events/s):**
```
Speed Layer:
- Processors: 3 ‚Üí 10 replicas
- Kafka partitions: 3 ‚Üí 12
- RDS: t3.micro ‚Üí t3.large
- Cost: +$200/month

Batch Layer:
- S3 storage: 10GB ‚Üí 100GB/month (+$2/month)
- Spark: Migrate to EMR cluster (+$100/month)
- Processing time: 3 min ‚Üí 15 min (still acceptable)
```

**100x Scale (50,000 events/s):**
```
Architecture changes required:
- Multi-region Kafka cluster
- Dedicated Spark cluster (EMR/Databricks)
- RDS read replicas (5-10 instances)
- CDN for dashboard
- Cost: ~$2000-3000/month
```

---

## üí∞ Cost Analysis

### Monthly Costs (Production - AWS us-east-1)

**Compute (EC2 t3.medium on-demand):**
```
Instance: $0.0416/hour √ó 730 hours = $30.37/month
Alternative (Spot): ~$9/month (70% savings)
```

**Database (RDS db.t3.micro):**
```
Instance: $0.017/hour √ó 730 hours = $12.41/month
Storage: 20GB √ó $0.115/GB = $2.30/month
Total RDS: $14.71/month
```

**Storage (S3 Standard):**
```
Data: 10GB √ó $0.023/GB = $0.23/month
Requests: Negligible (<$0.10/month)
Total S3: $0.33/month
```

**Network:**
```
Data transfer: <1GB/month (same region) = Free
Total: $0/month
```

**Total: ~$45.41/month (on-demand)**  
**Optimized: ~$24/month (with Spot instances)**

### Cost Optimization Strategies

1. **Use Spot Instances**: 70% savings on EC2
2. **RDS Reserved Instances**: 30-60% savings for 1-3 year commit
3. **S3 Intelligent-Tiering**: Auto-archive old data
4. **Right-sizing**: Monitor and adjust instance types
5. **Scheduled Scaling**: Scale down during off-peak hours

---

## üõ°Ô∏è Reliability & Fault Tolerance

### High Availability Design

**Component Redundancy:**
```
‚úÖ Producer: Single point, but stateless (fast restart)
‚úÖ Kafka: Data persisted, survives crashes
‚úÖ Processors: 3 replicas, consumer group balancing
‚úÖ Database: RDS managed, auto-backup
‚úÖ S3: 11 nines durability, cross-AZ replication
```

**Failure Scenarios:**

| Component | Impact | Recovery Time | Data Loss |
|-----------|--------|---------------|-----------|
| Producer crash | No new events | 30s (restart) | Minimal |
| Kafka down | Pipeline halts | 1-2 min | None (persistent) |
| 1 processor crash | 33% throughput drop | 30s (rebalance) | None (offset) |
| All processors down | No DB writes | 1-2 min | Events buffered in Kafka |
| RDS unavailable | Dashboard + writes fail | 5-10 min (failover) | None (replicated) |
| S3 unavailable | Batch ingestion fails | Minutes to hours | Events in Kafka buffer |

**Monitoring & Alerting:**
```yaml
Critical Alerts:
  - Consumer lag > 10,000 events
  - No events received for 5 minutes
  - Database CPU > 90%
  - RDS disk space > 90%

Warning Alerts:
  - Consumer lag > 5,000 events
  - Database CPU > 70%
  - Error rate > 1%
  - Batch job duration > 30 minutes
```

---

## üîß Operations & Maintenance

### Daily Operations

**Morning Health Check:**
```bash
# Check all services
docker compose ps

# Verify data freshness
psql -c "SELECT MAX(event_time) FROM realtime_recent_changes;"
psql -c "SELECT MAX(hour_bucket) FROM historical_hourly_patterns;"

# Check consumer lag
docker exec redpanda rpk group describe wiki-quix-group
```

**Monitoring Commands:**
```bash
# Real-time logs
docker compose logs -f processing

# Error grep
docker compose logs processing | grep -i error

# Resource usage
docker stats

# Database connections
psql -c "SELECT count(*) FROM pg_stat_activity;"
```

### Weekly Maintenance

```bash
# Restart containers (fresh state)
docker compose restart processing

# Check S3 storage growth
aws s3 ls s3://bucket/raw_data/ --recursive --summarize

# Vacuum database
psql -c "VACUUM ANALYZE realtime_recent_changes;"
psql -c "VACUUM ANALYZE historical_hourly_patterns;"
```

### Monthly Tasks

```sql
-- Review table sizes
SELECT 
  schemaname, tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables 
WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Check index usage
SELECT 
  schemaname, tablename, indexname,
  idx_scan, idx_tup_read, idx_tup_fetch
FROM pg_stat_user_indexes
ORDER BY idx_scan;
```

---

## üöÄ Deployment Guide

### Prerequisites

```bash
# AWS Account with:
- EC2 instance (t3.medium recommended)
- RDS PostgreSQL instance
- S3 bucket
- IAM role with S3 + RDS permissions

# Local tools:
- Docker & Docker Compose
- Git
- AWS CLI (configured)
```

### Step-by-Step Deployment

**1. Clone Repository:**
```bash
git clone https://github.com/Thanh14013/wiki_data_analysis.git
cd wiki_data_analysis
```

**2. Configure Environment:**
```bash
cp .env.example .env
nano .env

# Update:
POSTGRES_HOST=your-rds-endpoint.rds.amazonaws.com
S3_BUCKET_NAME=your-bucket-name
DATA_LAKE_PATH=s3a://your-bucket/raw_data
```

**3. Initialize Database:**
```bash
# Connect to RDS
psql -h $POSTGRES_HOST -U $POSTGRES_USER -d $POSTGRES_DB

# Run schema creation
\i scripts/create_tables_rds.py
```

**4. Start Services:**
```bash
docker compose up -d --build

# Verify all containers running
docker compose ps
```

**5. Verify Data Flow:**
```bash
# Check producer
docker compose logs producer --tail=50
# Should see: "Sent X messages"

# Check processing
docker compose logs processing --tail=50
# Should see: "‚úÖ Wrote N records"

# Check database
psql -c "SELECT COUNT(*) FROM realtime_recent_changes;"
```

**6. Access Dashboard:**
```
Open browser: http://your-ec2-ip:8501
```

---

## üìö Documentation References

- **Speed Layer Details**: [docs/STREAMING.md](docs/STREAMING.md)
- **Batch Layer Details**: [docs/BATCH.md](docs/BATCH.md)
- **Dashboard Guide**: [DASHBOARD_REPORT.md](DASHBOARD_REPORT.md)
- **API Documentation**: Inline code comments
- **Troubleshooting**: See each layer's documentation

---

## üéØ Future Roadmap

### Q1 2026 (Current Focus)
- ‚úÖ Lambda Architecture implementation
- ‚úÖ S3 Data Lake integration
- ‚úÖ Spark batch processing
- ‚è≥ Airflow orchestration
- ‚è≥ Prometheus monitoring

### Q2 2026
- Schema Registry (Avro)
- Exactly-once semantics
- Multi-region deployment
- ML anomaly detection

### Q3-Q4 2026
- Flink/Spark Streaming migration
- Data catalog (AWS Glue)
- GraphQL API
- Mobile dashboard

---

**Document Status**: ‚úÖ Complete and Production-Ready  
**Last Validated**: January 15, 2026  
**Next Review**: February 2026
